{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md')\n",
    "# ! python3 -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_text(texts):\n",
    "#     processed_texts = []\n",
    "#     lemmatizer = WordNetLemmatizer()\n",
    "#     for text in texts:\n",
    "#         text = text.lower()\n",
    "#         # Tokenize sentences and words\n",
    "#         sentences = word_tokenize(text)\n",
    "#         tokens = [sent_tokenize(sentence) for sentence in sentences]\n",
    "#         # Lemmatize tokens\n",
    "#         tokens = [[lemmatizer.lemmatize(word) for word in sentence] for sentence in tokens]\n",
    "#         # Flatten and clean up tokens\n",
    "#         processed_sentences = [\" \".join(sentence) for sentence in tokens]\n",
    "#         processed_texts.append(processed_sentences)\n",
    "#     return processed_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(texts):\n",
    "    processed_texts = []\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    for text in texts:\n",
    "        text = text.lower()  # Convert to lowercase\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "        sentences = sent_tokenize(text)  # Tokenize into sentences\n",
    "        tokens = [word_tokenize(sentence) for sentence in sentences]  # Tokenize each sentence into words\n",
    "        tokens = [[lemmatizer.lemmatize(word) for word in sentence] for sentence in tokens]  # Lemmatize each word\n",
    "        # Flatten sentences into a single list of words for the whole document\n",
    "        flat_tokens = [word for sentence in tokens for word in sentence]\n",
    "        processed_texts.append(\" \".join(flat_tokens))  # Join tokens back into a single string\n",
    "    return processed_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['brick work supplying preparing and construction of chimney made brick work of approved quality with 12 chuna white cement surkhi mortar ratio with 30 m lead',\n",
       " '124 pcc work']"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b =  [\"Brick Work Supplying preparing and construction of chimney made brick work of approved quality with 1:2 chuna ( white cement) : Surkhi mortar ratio with 30 m lead\",\n",
    "    \"1:2:4 PCC Works\",]\n",
    "\n",
    "a = preprocess_text(b)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['brick work supplying preparing and construction of chimney made brick work of approved quality with 12 chuna white cement surkhi mortar ratio with 30 m lead',\n",
       " '124 pcc work']"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_texts = [\"Brick Work Supplying preparing and construction of chimney made brick work of approved quality with 1:2 chuna ( white cement) : Surkhi mortar ratio with 30 m lead\",\n",
    "    \"1:2:4 PCC Works\",]\n",
    "sample_sentences_processed = preprocess_text(sample_texts)\n",
    "sample_sentences_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sample_sentences_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to a vector using spaCy\n",
    "def document_to_vector_spacy(doc_text, nlp_model):\n",
    "    doc = nlp_model(doc_text)\n",
    "    return doc.vector\n",
    "\n",
    "# Compute vectors for a list of documents\n",
    "def compute_spacy_vectors(documents, nlp_model):\n",
    "    vectors = [document_to_vector_spacy(doc, nlp_model) for doc in documents]\n",
    "    return np.array(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cosine_similarity(vector1, vector2):\n",
    "    similarity = cosine_similarity([vector1], [vector2])\n",
    "    return similarity[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in sample_sentences_processed:\n",
    "    sample_vector = compute_spacy_vectors(sentence , nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 300)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
